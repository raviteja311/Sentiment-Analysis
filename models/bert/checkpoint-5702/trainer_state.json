{
  "best_global_step": 5702,
  "best_metric": 0.7805747389723933,
  "best_model_checkpoint": "models/bert\\checkpoint-5702",
  "epoch": 1.0,
  "eval_steps": 500,
  "global_step": 5702,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.008768853034023149,
      "grad_norm": 9.269451141357422,
      "learning_rate": 1.9942710160177717e-05,
      "loss": 0.5719,
      "step": 50
    },
    {
      "epoch": 0.017537706068046298,
      "grad_norm": 4.063092231750488,
      "learning_rate": 1.9884251139950898e-05,
      "loss": 0.5096,
      "step": 100
    },
    {
      "epoch": 0.02630655910206945,
      "grad_norm": 14.730921745300293,
      "learning_rate": 1.9825792119724076e-05,
      "loss": 0.543,
      "step": 150
    },
    {
      "epoch": 0.035075412136092596,
      "grad_norm": 17.481151580810547,
      "learning_rate": 1.9767333099497254e-05,
      "loss": 0.5779,
      "step": 200
    },
    {
      "epoch": 0.04384426517011575,
      "grad_norm": 35.991416931152344,
      "learning_rate": 1.970887407927043e-05,
      "loss": 0.535,
      "step": 250
    },
    {
      "epoch": 0.0526131182041389,
      "grad_norm": 18.323360443115234,
      "learning_rate": 1.9650415059043612e-05,
      "loss": 0.5148,
      "step": 300
    },
    {
      "epoch": 0.061381971238162046,
      "grad_norm": 22.36538314819336,
      "learning_rate": 1.959195603881679e-05,
      "loss": 0.5384,
      "step": 350
    },
    {
      "epoch": 0.07015082427218519,
      "grad_norm": 9.89327335357666,
      "learning_rate": 1.953349701858997e-05,
      "loss": 0.6182,
      "step": 400
    },
    {
      "epoch": 0.07891967730620834,
      "grad_norm": 11.813533782958984,
      "learning_rate": 1.947503799836315e-05,
      "loss": 0.5684,
      "step": 450
    },
    {
      "epoch": 0.0876885303402315,
      "grad_norm": 11.302872657775879,
      "learning_rate": 1.941657897813633e-05,
      "loss": 0.4822,
      "step": 500
    },
    {
      "epoch": 0.09645738337425465,
      "grad_norm": 17.465103149414062,
      "learning_rate": 1.9358119957909508e-05,
      "loss": 0.5804,
      "step": 550
    },
    {
      "epoch": 0.1052262364082778,
      "grad_norm": 19.91106414794922,
      "learning_rate": 1.9299660937682686e-05,
      "loss": 0.4925,
      "step": 600
    },
    {
      "epoch": 0.11399508944230095,
      "grad_norm": 10.399276733398438,
      "learning_rate": 1.9241201917455864e-05,
      "loss": 0.5983,
      "step": 650
    },
    {
      "epoch": 0.12276394247632409,
      "grad_norm": 11.926600456237793,
      "learning_rate": 1.9182742897229045e-05,
      "loss": 0.5741,
      "step": 700
    },
    {
      "epoch": 0.13153279551034724,
      "grad_norm": 17.118207931518555,
      "learning_rate": 1.9124283877002222e-05,
      "loss": 0.5936,
      "step": 750
    },
    {
      "epoch": 0.14030164854437038,
      "grad_norm": 17.661211013793945,
      "learning_rate": 1.90658248567754e-05,
      "loss": 0.5892,
      "step": 800
    },
    {
      "epoch": 0.14907050157839355,
      "grad_norm": 19.39344596862793,
      "learning_rate": 1.900736583654858e-05,
      "loss": 0.5185,
      "step": 850
    },
    {
      "epoch": 0.1578393546124167,
      "grad_norm": 18.833372116088867,
      "learning_rate": 1.894890681632176e-05,
      "loss": 0.5458,
      "step": 900
    },
    {
      "epoch": 0.16660820764643985,
      "grad_norm": 11.862829208374023,
      "learning_rate": 1.889044779609494e-05,
      "loss": 0.5683,
      "step": 950
    },
    {
      "epoch": 0.175377060680463,
      "grad_norm": 12.708547592163086,
      "learning_rate": 1.8831988775868118e-05,
      "loss": 0.5946,
      "step": 1000
    },
    {
      "epoch": 0.18414591371448616,
      "grad_norm": 6.447486400604248,
      "learning_rate": 1.87735297556413e-05,
      "loss": 0.638,
      "step": 1050
    },
    {
      "epoch": 0.1929147667485093,
      "grad_norm": 37.22871780395508,
      "learning_rate": 1.8715070735414477e-05,
      "loss": 0.5876,
      "step": 1100
    },
    {
      "epoch": 0.20168361978253244,
      "grad_norm": 42.21690368652344,
      "learning_rate": 1.8656611715187655e-05,
      "loss": 0.5677,
      "step": 1150
    },
    {
      "epoch": 0.2104524728165556,
      "grad_norm": 11.182705879211426,
      "learning_rate": 1.8598152694960832e-05,
      "loss": 0.5613,
      "step": 1200
    },
    {
      "epoch": 0.21922132585057874,
      "grad_norm": 26.606956481933594,
      "learning_rate": 1.8539693674734014e-05,
      "loss": 0.5852,
      "step": 1250
    },
    {
      "epoch": 0.2279901788846019,
      "grad_norm": 23.74173355102539,
      "learning_rate": 1.848123465450719e-05,
      "loss": 0.6553,
      "step": 1300
    },
    {
      "epoch": 0.23675903191862505,
      "grad_norm": 14.445143699645996,
      "learning_rate": 1.842277563428037e-05,
      "loss": 0.5343,
      "step": 1350
    },
    {
      "epoch": 0.24552788495264818,
      "grad_norm": 18.762821197509766,
      "learning_rate": 1.836431661405355e-05,
      "loss": 0.589,
      "step": 1400
    },
    {
      "epoch": 0.25429673798667135,
      "grad_norm": 12.612329483032227,
      "learning_rate": 1.8305857593826728e-05,
      "loss": 0.5086,
      "step": 1450
    },
    {
      "epoch": 0.2630655910206945,
      "grad_norm": 14.593262672424316,
      "learning_rate": 1.824739857359991e-05,
      "loss": 0.6132,
      "step": 1500
    },
    {
      "epoch": 0.2718344440547176,
      "grad_norm": 22.87300682067871,
      "learning_rate": 1.8188939553373087e-05,
      "loss": 0.464,
      "step": 1550
    },
    {
      "epoch": 0.28060329708874077,
      "grad_norm": 27.788814544677734,
      "learning_rate": 1.8130480533146265e-05,
      "loss": 0.6391,
      "step": 1600
    },
    {
      "epoch": 0.28937215012276396,
      "grad_norm": 12.783684730529785,
      "learning_rate": 1.8072021512919446e-05,
      "loss": 0.5467,
      "step": 1650
    },
    {
      "epoch": 0.2981410031567871,
      "grad_norm": 20.185020446777344,
      "learning_rate": 1.8013562492692624e-05,
      "loss": 0.517,
      "step": 1700
    },
    {
      "epoch": 0.30690985619081024,
      "grad_norm": 13.459314346313477,
      "learning_rate": 1.79551034724658e-05,
      "loss": 0.6349,
      "step": 1750
    },
    {
      "epoch": 0.3156787092248334,
      "grad_norm": 9.261687278747559,
      "learning_rate": 1.7896644452238982e-05,
      "loss": 0.5552,
      "step": 1800
    },
    {
      "epoch": 0.3244475622588565,
      "grad_norm": 8.153563499450684,
      "learning_rate": 1.783818543201216e-05,
      "loss": 0.5144,
      "step": 1850
    },
    {
      "epoch": 0.3332164152928797,
      "grad_norm": 29.9373722076416,
      "learning_rate": 1.777972641178534e-05,
      "loss": 0.5064,
      "step": 1900
    },
    {
      "epoch": 0.34198526832690285,
      "grad_norm": 10.64893627166748,
      "learning_rate": 1.772126739155852e-05,
      "loss": 0.5561,
      "step": 1950
    },
    {
      "epoch": 0.350754121360926,
      "grad_norm": 39.572147369384766,
      "learning_rate": 1.76628083713317e-05,
      "loss": 0.5833,
      "step": 2000
    },
    {
      "epoch": 0.3595229743949491,
      "grad_norm": 34.190162658691406,
      "learning_rate": 1.7604349351104878e-05,
      "loss": 0.5438,
      "step": 2050
    },
    {
      "epoch": 0.3682918274289723,
      "grad_norm": 8.484058380126953,
      "learning_rate": 1.7545890330878056e-05,
      "loss": 0.5719,
      "step": 2100
    },
    {
      "epoch": 0.37706068046299546,
      "grad_norm": 24.56251335144043,
      "learning_rate": 1.7487431310651234e-05,
      "loss": 0.506,
      "step": 2150
    },
    {
      "epoch": 0.3858295334970186,
      "grad_norm": 22.614343643188477,
      "learning_rate": 1.742897229042441e-05,
      "loss": 0.4917,
      "step": 2200
    },
    {
      "epoch": 0.39459838653104173,
      "grad_norm": 25.911060333251953,
      "learning_rate": 1.7370513270197592e-05,
      "loss": 0.5379,
      "step": 2250
    },
    {
      "epoch": 0.4033672395650649,
      "grad_norm": 23.40053939819336,
      "learning_rate": 1.731205424997077e-05,
      "loss": 0.5787,
      "step": 2300
    },
    {
      "epoch": 0.41213609259908807,
      "grad_norm": 23.049266815185547,
      "learning_rate": 1.725359522974395e-05,
      "loss": 0.6302,
      "step": 2350
    },
    {
      "epoch": 0.4209049456331112,
      "grad_norm": 27.825387954711914,
      "learning_rate": 1.719513620951713e-05,
      "loss": 0.5667,
      "step": 2400
    },
    {
      "epoch": 0.42967379866713434,
      "grad_norm": 5.987380027770996,
      "learning_rate": 1.713667718929031e-05,
      "loss": 0.4961,
      "step": 2450
    },
    {
      "epoch": 0.4384426517011575,
      "grad_norm": 27.40089225769043,
      "learning_rate": 1.7078218169063488e-05,
      "loss": 0.6228,
      "step": 2500
    },
    {
      "epoch": 0.4472115047351806,
      "grad_norm": 20.172792434692383,
      "learning_rate": 1.701975914883667e-05,
      "loss": 0.5113,
      "step": 2550
    },
    {
      "epoch": 0.4559803577692038,
      "grad_norm": 12.78254222869873,
      "learning_rate": 1.6961300128609847e-05,
      "loss": 0.5665,
      "step": 2600
    },
    {
      "epoch": 0.46474921080322695,
      "grad_norm": 21.279376983642578,
      "learning_rate": 1.6902841108383025e-05,
      "loss": 0.5743,
      "step": 2650
    },
    {
      "epoch": 0.4735180638372501,
      "grad_norm": 7.306910991668701,
      "learning_rate": 1.6844382088156202e-05,
      "loss": 0.5124,
      "step": 2700
    },
    {
      "epoch": 0.48228691687127323,
      "grad_norm": 17.388811111450195,
      "learning_rate": 1.6785923067929384e-05,
      "loss": 0.6083,
      "step": 2750
    },
    {
      "epoch": 0.49105576990529637,
      "grad_norm": 11.664456367492676,
      "learning_rate": 1.672746404770256e-05,
      "loss": 0.577,
      "step": 2800
    },
    {
      "epoch": 0.49982462293931956,
      "grad_norm": 32.488555908203125,
      "learning_rate": 1.666900502747574e-05,
      "loss": 0.4952,
      "step": 2850
    },
    {
      "epoch": 0.5085934759733427,
      "grad_norm": 18.25028419494629,
      "learning_rate": 1.661054600724892e-05,
      "loss": 0.5554,
      "step": 2900
    },
    {
      "epoch": 0.5173623290073658,
      "grad_norm": 5.875308990478516,
      "learning_rate": 1.6552086987022098e-05,
      "loss": 0.5322,
      "step": 2950
    },
    {
      "epoch": 0.526131182041389,
      "grad_norm": 7.058801651000977,
      "learning_rate": 1.649362796679528e-05,
      "loss": 0.5685,
      "step": 3000
    },
    {
      "epoch": 0.5349000350754122,
      "grad_norm": 14.5927095413208,
      "learning_rate": 1.6435168946568457e-05,
      "loss": 0.5539,
      "step": 3050
    },
    {
      "epoch": 0.5436688881094353,
      "grad_norm": 17.978851318359375,
      "learning_rate": 1.6376709926341635e-05,
      "loss": 0.5812,
      "step": 3100
    },
    {
      "epoch": 0.5524377411434584,
      "grad_norm": 23.95724105834961,
      "learning_rate": 1.6318250906114816e-05,
      "loss": 0.4787,
      "step": 3150
    },
    {
      "epoch": 0.5612065941774815,
      "grad_norm": 13.50792407989502,
      "learning_rate": 1.6259791885887994e-05,
      "loss": 0.5762,
      "step": 3200
    },
    {
      "epoch": 0.5699754472115047,
      "grad_norm": 23.8729190826416,
      "learning_rate": 1.620133286566117e-05,
      "loss": 0.5064,
      "step": 3250
    },
    {
      "epoch": 0.5787443002455279,
      "grad_norm": 9.210742950439453,
      "learning_rate": 1.6142873845434353e-05,
      "loss": 0.5452,
      "step": 3300
    },
    {
      "epoch": 0.587513153279551,
      "grad_norm": 11.12401294708252,
      "learning_rate": 1.608441482520753e-05,
      "loss": 0.567,
      "step": 3350
    },
    {
      "epoch": 0.5962820063135742,
      "grad_norm": 28.57172203063965,
      "learning_rate": 1.602595580498071e-05,
      "loss": 0.5299,
      "step": 3400
    },
    {
      "epoch": 0.6050508593475973,
      "grad_norm": 20.411325454711914,
      "learning_rate": 1.596749678475389e-05,
      "loss": 0.5178,
      "step": 3450
    },
    {
      "epoch": 0.6138197123816205,
      "grad_norm": 19.424659729003906,
      "learning_rate": 1.590903776452707e-05,
      "loss": 0.5941,
      "step": 3500
    },
    {
      "epoch": 0.6225885654156437,
      "grad_norm": 15.695929527282715,
      "learning_rate": 1.5850578744300248e-05,
      "loss": 0.5515,
      "step": 3550
    },
    {
      "epoch": 0.6313574184496668,
      "grad_norm": 21.811094284057617,
      "learning_rate": 1.5792119724073426e-05,
      "loss": 0.5619,
      "step": 3600
    },
    {
      "epoch": 0.64012627148369,
      "grad_norm": 45.32130813598633,
      "learning_rate": 1.5733660703846604e-05,
      "loss": 0.5335,
      "step": 3650
    },
    {
      "epoch": 0.648895124517713,
      "grad_norm": 22.707542419433594,
      "learning_rate": 1.567520168361978e-05,
      "loss": 0.5493,
      "step": 3700
    },
    {
      "epoch": 0.6576639775517362,
      "grad_norm": 33.2109375,
      "learning_rate": 1.5616742663392962e-05,
      "loss": 0.5691,
      "step": 3750
    },
    {
      "epoch": 0.6664328305857594,
      "grad_norm": 16.81756591796875,
      "learning_rate": 1.555828364316614e-05,
      "loss": 0.5369,
      "step": 3800
    },
    {
      "epoch": 0.6752016836197825,
      "grad_norm": 22.336523056030273,
      "learning_rate": 1.549982462293932e-05,
      "loss": 0.4674,
      "step": 3850
    },
    {
      "epoch": 0.6839705366538057,
      "grad_norm": 11.336613655090332,
      "learning_rate": 1.54413656027125e-05,
      "loss": 0.6177,
      "step": 3900
    },
    {
      "epoch": 0.6927393896878288,
      "grad_norm": 12.028944969177246,
      "learning_rate": 1.538290658248568e-05,
      "loss": 0.5452,
      "step": 3950
    },
    {
      "epoch": 0.701508242721852,
      "grad_norm": 16.315956115722656,
      "learning_rate": 1.5324447562258858e-05,
      "loss": 0.6158,
      "step": 4000
    },
    {
      "epoch": 0.7102770957558752,
      "grad_norm": 14.806061744689941,
      "learning_rate": 1.5265988542032036e-05,
      "loss": 0.4939,
      "step": 4050
    },
    {
      "epoch": 0.7190459487898982,
      "grad_norm": 21.238037109375,
      "learning_rate": 1.5207529521805215e-05,
      "loss": 0.5083,
      "step": 4100
    },
    {
      "epoch": 0.7278148018239214,
      "grad_norm": 19.869348526000977,
      "learning_rate": 1.5149070501578396e-05,
      "loss": 0.576,
      "step": 4150
    },
    {
      "epoch": 0.7365836548579446,
      "grad_norm": 27.60514259338379,
      "learning_rate": 1.5090611481351574e-05,
      "loss": 0.5515,
      "step": 4200
    },
    {
      "epoch": 0.7453525078919677,
      "grad_norm": 3.3069827556610107,
      "learning_rate": 1.5032152461124754e-05,
      "loss": 0.5304,
      "step": 4250
    },
    {
      "epoch": 0.7541213609259909,
      "grad_norm": 14.032247543334961,
      "learning_rate": 1.4973693440897931e-05,
      "loss": 0.5435,
      "step": 4300
    },
    {
      "epoch": 0.762890213960014,
      "grad_norm": 4.6077046394348145,
      "learning_rate": 1.4915234420671109e-05,
      "loss": 0.6205,
      "step": 4350
    },
    {
      "epoch": 0.7716590669940372,
      "grad_norm": 22.202877044677734,
      "learning_rate": 1.485677540044429e-05,
      "loss": 0.5564,
      "step": 4400
    },
    {
      "epoch": 0.7804279200280604,
      "grad_norm": 20.091707229614258,
      "learning_rate": 1.4798316380217468e-05,
      "loss": 0.5128,
      "step": 4450
    },
    {
      "epoch": 0.7891967730620835,
      "grad_norm": 16.959680557250977,
      "learning_rate": 1.4739857359990648e-05,
      "loss": 0.5554,
      "step": 4500
    },
    {
      "epoch": 0.7979656260961067,
      "grad_norm": 32.54659652709961,
      "learning_rate": 1.4681398339763825e-05,
      "loss": 0.5405,
      "step": 4550
    },
    {
      "epoch": 0.8067344791301297,
      "grad_norm": 20.55562973022461,
      "learning_rate": 1.4622939319537006e-05,
      "loss": 0.5631,
      "step": 4600
    },
    {
      "epoch": 0.8155033321641529,
      "grad_norm": 12.761007308959961,
      "learning_rate": 1.4564480299310184e-05,
      "loss": 0.5248,
      "step": 4650
    },
    {
      "epoch": 0.8242721851981761,
      "grad_norm": 19.39558982849121,
      "learning_rate": 1.4506021279083364e-05,
      "loss": 0.5092,
      "step": 4700
    },
    {
      "epoch": 0.8330410382321992,
      "grad_norm": 12.880406379699707,
      "learning_rate": 1.4447562258856543e-05,
      "loss": 0.5431,
      "step": 4750
    },
    {
      "epoch": 0.8418098912662224,
      "grad_norm": 49.78459548950195,
      "learning_rate": 1.4389103238629723e-05,
      "loss": 0.5618,
      "step": 4800
    },
    {
      "epoch": 0.8505787443002455,
      "grad_norm": 29.866716384887695,
      "learning_rate": 1.43306442184029e-05,
      "loss": 0.4675,
      "step": 4850
    },
    {
      "epoch": 0.8593475973342687,
      "grad_norm": 13.00359058380127,
      "learning_rate": 1.4272185198176081e-05,
      "loss": 0.5232,
      "step": 4900
    },
    {
      "epoch": 0.8681164503682919,
      "grad_norm": 9.941661834716797,
      "learning_rate": 1.421372617794926e-05,
      "loss": 0.5796,
      "step": 4950
    },
    {
      "epoch": 0.876885303402315,
      "grad_norm": 33.11953353881836,
      "learning_rate": 1.4155267157722439e-05,
      "loss": 0.5015,
      "step": 5000
    },
    {
      "epoch": 0.8856541564363382,
      "grad_norm": 9.219670295715332,
      "learning_rate": 1.4096808137495616e-05,
      "loss": 0.5629,
      "step": 5050
    },
    {
      "epoch": 0.8944230094703612,
      "grad_norm": 20.85619354248047,
      "learning_rate": 1.4038349117268794e-05,
      "loss": 0.6097,
      "step": 5100
    },
    {
      "epoch": 0.9031918625043844,
      "grad_norm": 25.27180290222168,
      "learning_rate": 1.3979890097041975e-05,
      "loss": 0.5635,
      "step": 5150
    },
    {
      "epoch": 0.9119607155384076,
      "grad_norm": 11.161294937133789,
      "learning_rate": 1.3921431076815153e-05,
      "loss": 0.555,
      "step": 5200
    },
    {
      "epoch": 0.9207295685724307,
      "grad_norm": 21.620391845703125,
      "learning_rate": 1.3862972056588333e-05,
      "loss": 0.4768,
      "step": 5250
    },
    {
      "epoch": 0.9294984216064539,
      "grad_norm": 32.37575149536133,
      "learning_rate": 1.380451303636151e-05,
      "loss": 0.535,
      "step": 5300
    },
    {
      "epoch": 0.938267274640477,
      "grad_norm": 19.793848037719727,
      "learning_rate": 1.3746054016134691e-05,
      "loss": 0.515,
      "step": 5350
    },
    {
      "epoch": 0.9470361276745002,
      "grad_norm": 14.29688549041748,
      "learning_rate": 1.368759499590787e-05,
      "loss": 0.5106,
      "step": 5400
    },
    {
      "epoch": 0.9558049807085234,
      "grad_norm": 7.229643821716309,
      "learning_rate": 1.3629135975681049e-05,
      "loss": 0.4879,
      "step": 5450
    },
    {
      "epoch": 0.9645738337425465,
      "grad_norm": 19.01701545715332,
      "learning_rate": 1.3570676955454228e-05,
      "loss": 0.5309,
      "step": 5500
    },
    {
      "epoch": 0.9733426867765697,
      "grad_norm": 14.163739204406738,
      "learning_rate": 1.3512217935227408e-05,
      "loss": 0.5479,
      "step": 5550
    },
    {
      "epoch": 0.9821115398105927,
      "grad_norm": 23.07871437072754,
      "learning_rate": 1.3453758915000585e-05,
      "loss": 0.5499,
      "step": 5600
    },
    {
      "epoch": 0.9908803928446159,
      "grad_norm": 15.059358596801758,
      "learning_rate": 1.3395299894773765e-05,
      "loss": 0.5111,
      "step": 5650
    },
    {
      "epoch": 0.9996492458786391,
      "grad_norm": 12.024609565734863,
      "learning_rate": 1.3336840874546944e-05,
      "loss": 0.5256,
      "step": 5700
    },
    {
      "epoch": 1.0,
      "eval_accuracy": 0.789,
      "eval_f1_macro": 0.7805747389723933,
      "eval_loss": 0.5145531296730042,
      "eval_runtime": 70.2761,
      "eval_samples_per_second": 28.459,
      "eval_steps_per_second": 3.557,
      "step": 5702
    }
  ],
  "logging_steps": 50,
  "max_steps": 17106,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 980163259391232.0,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
